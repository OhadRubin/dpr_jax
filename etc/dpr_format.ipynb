{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import numpy as np\n",
    "\n",
    "def unstack_element(element,n_examples=None):\n",
    "    keys = list(element.keys())\n",
    "    if n_examples is None:\n",
    "        n_examples = len(element[keys[0]])\n",
    "    for i in range(n_examples):\n",
    "        micro_element = {}\n",
    "        for key in keys:\n",
    "            try:\n",
    "                micro_element[key] = element[key][i]\n",
    "            except:\n",
    "                print([(key,len(element[key])) for key in keys])\n",
    "                raise\n",
    "        yield micro_element\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_examples(example,query_field=\"query\",pos_field=\"positive_passages\",neg_field=\"negative_passages\"):\n",
    "    tokenize = partial(tokenizer, return_attention_mask=False, return_token_type_ids=False, padding=True,\n",
    "                        truncation=True)\n",
    "    query = example[query_field]\n",
    "    pos_psgs = [p['title'] + \" \" + p['text'] for p in list(unstack_element(example[pos_field]))[:1]]\n",
    "    neg_psgs = [p['title'] + \" \" + p['text'] for p in list(unstack_element(example[neg_field]))[:9]]\n",
    "    def tok(x,l):\n",
    "        return dict(tokenize(x, max_length=l,padding='max_length', return_tensors='np'))[\"input_ids\"]\n",
    "        \n",
    "    query_input_ids = tok(query, 32)\n",
    "    psgs_input_ids = pos_psgs+neg_psgs\n",
    "    psgs_input_ids = [tok(x,128) for x in psgs_input_ids ] \n",
    "    psgs_input_ids = np.stack(psgs_input_ids)\n",
    "    \n",
    "\n",
    "    return dict(query_input_ids=query_input_ids, psgs_input_ids=psgs_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae93885363a4aa291b87ca7849d6db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6905b075a50c4b00a3ac00b9a1d13b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/294M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d8498850a146c0808cf6c2f38558b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236461eeb66b4977aff006f9317d77bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea13149c91d4e579afc401163c82ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset (num_proc=20):   0%|          | 0/4907 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "p=\"https://huggingface.co/datasets/iohadrubin/nq/resolve/main/data/train-00000-of-00012-aebee16ac9d5ed6f.parquet\"\n",
    "train_dataset = load_dataset(\"parquet\",data_files={\"train\":[p]},split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = train_dataset.map(\n",
    "    partial(tokenize_examples,query_field=\"question\",pos_field=\"positive_ctxs\",neg_field=\"hard_negative_ctxs\"),\n",
    "    batched=False,\n",
    "    num_proc=20,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IterableDatasetWrapper(IterableDataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(IterableDatasetWrapper).__init__()\n",
    "        self.dataset = dataset\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for x in self.dataset:\n",
    "                yield x\n",
    "            self.dataset = self.dataset.shuffle()\n",
    "\n",
    "def package(result):\n",
    "    keys = list(result[0].keys())\n",
    "    batch = {}\n",
    "    for key in keys:\n",
    "        batch[key] = np.array([res[key] for res in result]).squeeze(-2)\n",
    "    return batch   \n",
    "def get_dataloader(data, batch_size):\n",
    "    iterable = IterableDatasetWrapper(data) \n",
    "    dloader= DataLoader(iterable,\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=lambda v: package(v),\n",
    "                            num_workers=16, prefetch_factor=256,\n",
    "                            )\n",
    "    return dloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = get_dataloader(train_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_input_ids': array([[ 101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178,  102,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2040, 6369, 3403, 2005, 1037, 2611, 2066, 2017,  102,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'psgs_input_ids': array([[[  101,  2502,  2210, ...,   102,     0,     0],\n",
       "         [  101,  2210,  2111, ..., 18868,  1010,   102],\n",
       "         [  101, 15883,  2007, ...,  2285,  2418,   102],\n",
       "         ...,\n",
       "         [  101,  5487, 20996, ...,  1000,  1012,   102],\n",
       "         [  101,  2502,  2567, ...,  1015,  1012,   102],\n",
       "         [  101,  2129,  1045, ...,  2544,  1997,   102]],\n",
       " \n",
       "        [[  101,  3403,  2005, ...,  2316,  1005,   102],\n",
       "         [  101,  3403,  2005, ...,  2051,  1000,   102],\n",
       "         [  101,  3403,  2005, ...,  2281,  2654,   102],\n",
       "         ...,\n",
       "         [  101,  7656,  2957, ...,  2035,  2041,   102],\n",
       "         [  101,  2066,  1045, ...,  2009,  2074,   102],\n",
       "         [  101,  1996,  2397, ...,  2001,  3061,   102]]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_input_ids', 'psgs_input_ids'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178,  102,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [ 101, 2040, 6369, 3403, 2005, 1037, 2611, 2066, 2017,  102,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"query_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 128)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"psgs_input_ids\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
