{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BUCKET=meliad_eu2\n",
      "env: LOGURU_LEVEL=INFO\n",
      "/home/ohadr/dpr_jax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 10:51:58.228079: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2024-01-21 10:52:02.191814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2024-01-21 10:52:02.191883: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "%env BUCKET meliad_eu2\n",
    "%env LOGURU_LEVEL INFO\n",
    "%cd ~/dpr_jax\n",
    "from src.data import load_from_seqio\n",
    "import sys\n",
    "sys.path.append(\"/home/ohadr/meliad2\")\n",
    "dataset = load_from_seqio(\"codeparrot\",\"validation\",repeat=False,limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nei_token_ids': array([    0,     0,     0, ...,  2262,   187, 50274], dtype=int32),\n",
       " 'targets': array([    4, 45180, 12425, ...,    15,  8045,    64], dtype=int32),\n",
       " 'book_id': -436174858,\n",
       " 'nei_idx': array([ -1,  -1,  -1, ..., 196, 189,  50], dtype=int32),\n",
       " 'nei_scores': array([     -inf,      -inf,      -inf, ..., 24.954105, 23.405523,\n",
       "        23.344488], dtype=float32),\n",
       " 'neig': array([ 16,  15,   0, ..., 208,  50,  19], dtype=int32)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import numpy as np\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# tokenizer(text, truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = dataset.next()\n",
    "targets = element[\"targets\"]\n",
    "chunks = targets.reshape([-1,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenize = partial(tokenizer,\n",
    "                    return_attention_mask=True,\n",
    "                    return_token_type_ids=False,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    padding='max_length', return_tensors='np')\n",
    "    \n",
    "detokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "def encode_text(batch):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    detok_input_ids = detokenizer.batch_decode(input_ids)\n",
    "    detok_passage = [\"Passage: \"+x for x in detok_input_ids]\n",
    "    detok_question = [\"Question: \"+x for x in detok_input_ids]\n",
    "    passage_dict = dict(tokenize(detok_passage))\n",
    "    question_dict = dict(tokenize(detok_question))\n",
    "    passage_dict = {\"psgs_\"+k:v for k,v in passage_dict.items()}\n",
    "    question_dict = {\"query_\"+k:v for k,v in question_dict.items()}\n",
    "    batch.update(**passage_dict,**question_dict)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3b9712fc1c4583a9bc191485c4f1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.Dataset.from_generator(lambda: map(lambda x:dict(input_ids=x),chunks))\n",
    "ds = ds.map(encode_text, batched=True,remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def package(result):\n",
    "    keys = list(result[0].keys())\n",
    "    batch = {}\n",
    "    for key in keys:\n",
    "        try:\n",
    "            arr = np.array([res[key] for res in result])\n",
    "            batch[key] = arr\n",
    "        except ValueError:\n",
    "            print([np.array(res[key]).shape for res in result])\n",
    "            raise\n",
    "    query = {\"input_ids\":batch['query_input_ids'],\"attention_mask\":batch['query_attention_mask']}\n",
    "    psgs = {\"input_ids\":batch['psgs_input_ids'],\"attention_mask\":batch['psgs_attention_mask']}\n",
    "    return query,psgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader= DataLoader(ds,\n",
    "                        batch_size=10,\n",
    "                        collate_fn=lambda v: package(v)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_batch,p_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.01021561,  0.19040376,  1.6352711 , ...,  1.1421033 ,\n",
       "         0.95936346, -0.12786758],\n",
       "       [-0.34261122,  0.53752893,  1.1948521 , ...,  0.94131815,\n",
       "         0.85876745, -0.3351325 ],\n",
       "       [-0.25598946,  0.38868222,  1.1916975 , ...,  0.75775605,\n",
       "         0.65125734, -0.24994837],\n",
       "       ...,\n",
       "       [-0.18480799,  0.04126644,  1.4588346 , ...,  0.97078156,\n",
       "         0.7898591 , -0.07024911],\n",
       "       [ 0.02817565,  0.0380663 ,  1.590899  , ...,  0.91640073,\n",
       "         0.8452434 , -0.3121782 ],\n",
       "       [-0.3768375 ,  0.41357592,  1.4549042 , ...,  0.8485042 ,\n",
       "         0.9712858 , -0.5253505 ]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**p_batch)[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, FlaxAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxAutoModel.from_pretrained(\"/home/ohadr/dpr_jax/v7_n7_dscodeparrot_b20.95_wd0.01_steps100000/passage_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamTuple(q_params=model.params,p_params=model.params)\n",
    "state = RetrieverTrainState.create(apply_fn=model.__call__, params=params, tx=adamw)\n",
    "q_reps = state.apply_fn(**queries, params=params.q_params, dropout_rng=q_dropout_rng, train=True)[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage_input_ids', 'passage_token_type_ids', 'passage_attention_mask', 'question_input_ids', 'question_token_type_ids', 'question_attention_mask'],\n",
       "    num_rows: 209\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p=\"https://huggingface.co/datasets/iohadrubin/nq/resolve/main/data/train-00000-of-00012-aebee16ac9d5ed6f.parquet\"\n",
    "train_dataset = load_dataset(\"parquet\",data_files={\"train\":[p]},split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = train_dataset.map(\n",
    "    partial(tokenize_examples,query_field=\"question\",pos_field=\"positive_ctxs\",neg_field=\"hard_negative_ctxs\"),\n",
    "    batched=False,\n",
    "    num_proc=20,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94324a14410b463b84a9a6825458ff70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/4907 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs'],\n",
       "    num_rows: 4907\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.filter(lambda x: len(x['positive_ctxs']) > 0 and len(x['hard_negative_ctxs']) >= 5,\n",
    "                                         batched=False,\n",
    "                                         num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs'],\n",
       "    num_rows: 4907\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IterableDatasetWrapper(IterableDataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(IterableDatasetWrapper).__init__()\n",
    "        self.dataset = dataset\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for x in self.dataset:\n",
    "                yield x\n",
    "            self.dataset = self.dataset.shuffle()\n",
    "\n",
    "def package(result):\n",
    "    keys = list(result[0].keys())\n",
    "    batch = {}\n",
    "    for key in keys:\n",
    "        batch[key] = np.array([res[key] for res in result]).squeeze(-2)\n",
    "    return batch   \n",
    "def get_dataloader(data, batch_size):\n",
    "    iterable = IterableDatasetWrapper(data) \n",
    "    dloader= DataLoader(iterable,\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=lambda v: package(v),\n",
    "                            num_workers=16, prefetch_factor=256,\n",
    "                            )\n",
    "    return dloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_input_ids', 'psgs_input_ids'],\n",
       "    num_rows: 4907\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = get_dataloader(train_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_input_ids': array([[ 101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178,  102,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2040, 6369, 3403, 2005, 1037, 2611, 2066, 2017,  102,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'psgs_input_ids': array([[[  101,  2502,  2210, ...,   102,     0,     0],\n",
       "         [  101,  2210,  2111, ..., 18868,  1010,   102],\n",
       "         [  101, 15883,  2007, ...,  2285,  2418,   102],\n",
       "         ...,\n",
       "         [  101,  5487, 20996, ...,  1000,  1012,   102],\n",
       "         [  101,  2502,  2567, ...,  1015,  1012,   102],\n",
       "         [  101,  2129,  1045, ...,  2544,  1997,   102]],\n",
       " \n",
       "        [[  101,  3403,  2005, ...,  2316,  1005,   102],\n",
       "         [  101,  3403,  2005, ...,  2051,  1000,   102],\n",
       "         [  101,  3403,  2005, ...,  2281,  2654,   102],\n",
       "         ...,\n",
       "         [  101,  7656,  2957, ...,  2035,  2041,   102],\n",
       "         [  101,  2066,  1045, ...,  2009,  2074,   102],\n",
       "         [  101,  1996,  2397, ...,  2001,  3061,   102]]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_input_ids', 'psgs_input_ids'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178,  102,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [ 101, 2040, 6369, 3403, 2005, 1037, 2611, 2066, 2017,  102,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"query_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 128)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"psgs_input_ids\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
